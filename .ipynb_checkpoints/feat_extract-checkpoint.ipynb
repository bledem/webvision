{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\" Extract bottleneck features using neural network. \"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import sys\n",
    "import torch\n",
    "import pickle as cPickle\n",
    "from os.path import join as pjoin\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from cnn.factory import get_model\n",
    "from cnn.config import cfg\n",
    "from datasets.folder import SpecImageFolder\n",
    "from PIL import Image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \"/mnt/data1/webvision2.0\"\n",
    "batch_size = 100 \n",
    "input_size= 224\n",
    "arch=\"resnet50\"\n",
    "gpus =\"0,1,2,3\" \n",
    "data_folder =\"both\" \n",
    "save_freq = 5000\n",
    "num_classes= 5607 #number of class we want\n",
    "print_freq= 10000\n",
    "load_path ='/home/betty/webvision_train/results/resnet50/5000classes_onemonth/'\n",
    "num_workers = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "images_so_far = 0\n",
    "save_path='/home/betty/webvision_train/results/resnet50/training_features'\n",
    "params_file=\"/home/betty/webvision_train/results/resnet50/5000classes_onemonth/model_best.tar\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "queries=[]\n",
    "previous=-1\n",
    "all=True #to activate if you want all the query put in one class\n",
    "if (num_classes is None):\n",
    "    all=True\n",
    "with open(os.path.join(data_path, 'info', 'queries_synsets_map.txt')) as f:\n",
    "        for line in f:\n",
    "            # Image path\n",
    "            if((line.split()[1] != previous) or all):\n",
    "                queries.append(int(line.split()[0]))\n",
    "                previous=line.split()[1]\n",
    "                #print(line.split()[0], \" \", line.split()[1])\n",
    "            elif ((len(queries)>=query) and not all):\n",
    "                print(\"over the class\", query, 'queries is', queries)\n",
    "                break\n",
    "query_synset_dic = {}\n",
    "with open(\"/home/betty/Pictures/webvision/info/queries_synsets_map.txt\") as f:\n",
    "    for line in f:\n",
    "       (key, val) = line.split()\n",
    "       if (int(val)<=int(num_classes)):\n",
    "           query_synset_dic[int(key)] = val\n",
    "#print(\"We use \", query_synset_dic, \"queries as synset map\")\n",
    "# print(\"class weighted\", args.class_weighted )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_pickle(features, save_path, cls_id, fname):\n",
    "\n",
    "    with open(pjoin(save_path, cls_id, fname + \".pkl\"), 'wb') as f:\n",
    "        cPickle.dump(features, f, protocol=cPickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def save_to_txt(img_tuple, save_path, cls_id):\n",
    "    with open(pjoin(save_path, cls_id, \"img_name.lst\"), 'w') as f:\n",
    "        for img_path, _ in img_tuple:\n",
    "            f.write(img_path + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_feats(ext_loader, folder, model,\n",
    "                  save_path, cls_id,\n",
    "                  save_freq=200):\n",
    "    # switch to evaluate mode\n",
    "    save_path = pjoin(save_path, folder)\n",
    "    model.eval()\n",
    "    print(pjoin(save_path, cls_id))\n",
    "    if not os.path.exists(pjoin(save_path, cls_id)): #save the resulting feature\n",
    "        os.mkdir(pjoin(save_path, cls_id))\n",
    "\n",
    "    batch_feat = []\n",
    "    img_names = ext_loader.dataset.imgs\n",
    "    batch_size = ext_loader.batch_size\n",
    "    num_img = len(img_names)\n",
    "    init_idx = 0\n",
    "    pkl_idx = 0\n",
    "    last_idx = int(math.ceil(num_img / float(batch_size))) - 1\n",
    "\n",
    "    for idx, data in enumerate(ext_loader, 0):\n",
    "        inputs, _ = data\n",
    "        inputs = Variable(inputs.to(device))\n",
    "        feats = model(inputs)\n",
    "        print(\"check shape\", feats.shape)\n",
    "\n",
    "        cpu_feat = feats.data.cpu().numpy()\n",
    "        if len(cpu_feat.shape) == 1:\n",
    "            cpu_feat = np.reshape(cpu_feat, (1, -1))\n",
    "        batch_feat.append(cpu_feat)\n",
    "\n",
    "        if idx % save_freq == (save_freq - 1):\n",
    "            # batch_im_list = img_names[\n",
    "            #     init_idx: batch_size * save_freq + init_idx]\n",
    "            # init_idx = batch_size * save_freq + init_idx\n",
    "            batch_feat = np.concatenate(batch_feat, axis=0)\n",
    "            save_to_pickle(batch_feat, save_path, cls_id, str(pkl_idx))\n",
    "\n",
    "            batch_feat = []\n",
    "            pkl_idx += 1\n",
    "\n",
    "        elif idx == last_idx:\n",
    "            # batch_im_list = img_names[init_idx:]\n",
    "            batch_feat = np.concatenate(batch_feat, axis=0)\n",
    "\n",
    "            save_to_pickle(batch_feat, save_path, cls_id, str(pkl_idx))\n",
    "\n",
    "    # save to text\n",
    "    save_to_txt(img_names, save_path, cls_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_model(data_root, gpus=gpus, batch_size=16,\n",
    "                  params_file=\"/home/betty/webvision_train/results/resnet50/5000classes_onemonth/model_best.tar\",\n",
    "                  num_workers=4, num_classes=5000,\n",
    "                  in_size=224, save_path=None, dict_=None):\n",
    "    \n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    ext_transform = transforms.Compose([\n",
    "        transforms.Resize((in_size, in_size)),\n",
    "        transforms.ToTensor(),\n",
    "        normalize])\n",
    "    print(params_file)\n",
    "    assert os.path.isfile(params_file), \"{} is not exist.\".format(params_file)\n",
    "    # define the model\n",
    "    model = get_model(name=arch,\n",
    "                      num_classes=num_classes, extract_feat=True)\n",
    "    if len(gpus) > 1:\n",
    "        prev_gpus = gpus\n",
    "        gpus = [int(i) for i in gpus.strip('[]').split(',')]\n",
    "        print(\"Let's use\", gpus, \"on\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] =  prev_gpus #we give this number as the device id we use\n",
    "        gpus_idx = range(0,len(gpus))\n",
    "        model = torch.nn.DataParallel(model, device_ids=gpus_idx)\n",
    "    elif len(gpus) == 1:\n",
    "        prev_gpus = gpus\n",
    "        gpus = [int(i) for i in gpus.strip('[]').split(',')]\n",
    "        print(\"Let's use\", gpus, \"on\", torch.cuda.device_count(), \"GPUs!\")\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] =  prev_gpus #we give this number as the device id we use\n",
    "\n",
    "    else:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = gpus\n",
    "        print(\"no gpus\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.cuda()\n",
    "    params = torch.load(params_file)\n",
    "\n",
    "    model.load_state_dict(params['state_dict'])\n",
    "    #modules = list(model.children())[:-1]\n",
    "    #model =  nn.Sequential(*modules)\n",
    "    \n",
    "    ##un comment to PRINT the model\n",
    "#     print(\"Params to learn:\")\n",
    "#     feature_extract=False\n",
    "#     if feature_extract:\n",
    "#         params_to_update = []\n",
    "#         for name,param in model.named_parameters():\n",
    "#             if param.requires_grad == True:\n",
    "#                 params_to_update.append(param)\n",
    "#                 print(\"\\t\",name)\n",
    "#     else:\n",
    "#         for name,param in model.named_parameters():\n",
    "#             if param.requires_grad == True:\n",
    "#                 print(\"\\t\",name)\n",
    "    feature_map = list(model.module.children())\n",
    "    feature_map.pop()\n",
    "    extractor = nn.Sequential(*feature_map)\n",
    "    extractor.to(device)\n",
    "    extractor.cuda()\n",
    "    #print(\"feature_map\", feature_map)\n",
    "    for name,param in extractor.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "    \n",
    "#for every query\n",
    "    for cls_id in sorted(os.listdir(data_root)): #args.data_path  \n",
    "        print(\"Processing class {}\".format(cls_id))\n",
    "        ext_data = SpecImageFolder(root=pjoin(data_root, cls_id),\n",
    "                                   transform=ext_transform, dict_=dict_)\n",
    "        ext_loader = DataLoader(dataset=ext_data, batch_size=batch_size,\n",
    "                                shuffle=False, num_workers=num_workers,\n",
    "                                pin_memory=True)\n",
    "        extract_feats(ext_loader, data_root[24:30], extractor, save_path,  cls_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/betty/webvision_train/results/resnet50/5000classes_onemonth/model_best.tar\n",
      "Let's use [0, 1, 2, 3] on 4 GPUs!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 43.88 MiB (GPU 0; 7.92 GiB total capacity; 7.31 GiB already allocated; 17.00 MiB free; 9.77 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f984c25ebd80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                   \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                   save_path,dict_=query_synset_dic)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-160b394e167d>\u001b[0m in \u001b[0;36mextract_model\u001b[0;34m(data_root, gpus, batch_size, params_file, num_workers, num_classes, in_size, save_path, dict_)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"no gpus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 43.88 MiB (GPU 0; 7.92 GiB total capacity; 7.31 GiB already allocated; 17.00 MiB free; 9.77 MiB cached)"
     ]
    }
   ],
   "source": [
    "\n",
    "        \n",
    "google_path=\"/mnt/data1/webvision2.0/google_images_resized\"\n",
    "flickr_path=\"/mnt/data1/webvision2.0/flickr_images_resized\"\n",
    "extract_model(google_path,\n",
    "                  gpus,\n",
    "                  batch_size,\n",
    "                  params_file,\n",
    "                  num_workers,\n",
    "                  num_classes,\n",
    "                  input_size,\n",
    "                  save_path,dict_=query_synset_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
